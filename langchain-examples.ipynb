{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b211ba",
   "metadata": {},
   "source": [
    "# LangChain Hello World - Python Examples\n",
    "\n",
    "This notebook demonstrates various LangChain capabilities in Python with support for OpenAI GPT and Google Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Packages imported successfully!\")\n",
    "print(\"üìã Available providers:\")\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"  ‚úÖ OpenAI GPT\")\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"  ‚úÖ Google Gemini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db53df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize both LLMs (if API keys are available)\n",
    "openai_llm = None\n",
    "gemini_llm = None\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    openai_llm = ChatOpenAI(\n",
    "        temperature=0.7,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    print(\"‚úÖ OpenAI LLM initialized successfully!\")\n",
    "\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    gemini_llm = ChatGoogleGenerativeAI(\n",
    "        temperature=0.7,\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    print(\"‚úÖ Google Gemini LLM initialized successfully!\")\n",
    "\n",
    "# Default to available LLM\n",
    "llm = openai_llm if openai_llm else gemini_llm\n",
    "current_provider = \"OpenAI GPT\" if openai_llm and llm == openai_llm else \"Google Gemini\"\n",
    "print(f\"üîÑ Using: {current_provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfda338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic chat example\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that explains concepts simply.\"),\n",
    "    HumanMessage(content=\"What is LangChain and why is it useful?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(f\"ü§ñ {current_provider} Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27280fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation with memory\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"üí¨ Starting conversation with memory...\")\n",
    "response1 = conversation.predict(input=\"Hi! My name is John and I'm learning about AI.\")\n",
    "print(f\"Response 1: {response1}\")\n",
    "\n",
    "response2 = conversation.predict(input=\"What's my name?\")\n",
    "print(f\"Response 2: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69670049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test different prompts with both providers\n",
    "def test_prompt(prompt, provider_llm=None, provider_name=\"Default\"):\n",
    "    \"\"\"Test a prompt and return the response\"\"\"\n",
    "    if provider_llm is None:\n",
    "        provider_llm = llm\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a creative assistant.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    response = provider_llm.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "# Test different prompts\n",
    "prompts = [\n",
    "    \"Write a haiku about programming\",\n",
    "    \"Explain quantum computing in 50 words\",\n",
    "    \"What are the benefits of using LangChain?\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\nüìù Prompt {i}: {prompt}\")\n",
    "    print(f\"ü§ñ {current_provider} Response: {test_prompt(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cea43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare responses from both providers (if both are available)\n",
    "if openai_llm and gemini_llm:\n",
    "    test_question = \"Explain artificial intelligence in one sentence.\"\n",
    "    \n",
    "    print(\"üîÑ Comparing responses from both providers:\\n\")\n",
    "    print(f\"Question: {test_question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # OpenAI response\n",
    "    openai_response = test_prompt(test_question, openai_llm, \"OpenAI GPT\")\n",
    "    print(f\"ü§ñ OpenAI GPT: {openai_response}\")\n",
    "    print()\n",
    "    \n",
    "    # Gemini response\n",
    "    gemini_response = test_prompt(test_question, gemini_llm, \"Google Gemini\")\n",
    "    print(f\"ü§ñ Google Gemini: {gemini_response}\")\n",
    "    print()\n",
    "    \n",
    "    # Switch between providers\n",
    "    print(\"üîß You can switch providers by changing the 'llm' variable:\")\n",
    "    print(\"   llm = openai_llm   # Use OpenAI GPT\")\n",
    "    print(\"   llm = gemini_llm   # Use Google Gemini\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Only one provider is available. Add both API keys to compare responses!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1b30f",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Try switching between OpenAI GPT and Google Gemini by changing the `llm` variable\n",
    "- Compare responses from both providers using the comparison cell above\n",
    "- Modify the prompts and system messages to see how different providers respond\n",
    "- Experiment with different temperature values for each provider\n",
    "- Explore other LangChain components like chains, agents, and tools\n",
    "- Check out the LangChain documentation for more advanced features\n",
    "\n",
    "## Provider-Specific Features\n",
    "\n",
    "### OpenAI GPT\n",
    "- Supports multiple models (gpt-3.5-turbo, gpt-4, etc.)\n",
    "- Well-documented with extensive community support\n",
    "- Good for general-purpose tasks and reasoning\n",
    "\n",
    "### Google Gemini\n",
    "- Native multimodal capabilities (text, images, etc.)\n",
    "- Fast response times with Gemini Flash\n",
    "- Integrated with Google's ecosystem"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
